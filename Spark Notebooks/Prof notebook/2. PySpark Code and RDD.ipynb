{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#print(sum)\" data-toc-modified-id=\"print(sum)-1\">print(sum)</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T16:14:46.580985Z",
     "start_time": "2021-01-04T16:14:46.570555Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algae Services\n"
     ]
    }
   ],
   "source": [
    "print ('Algae Services')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T16:14:50.427055Z",
     "start_time": "2021-01-04T16:14:50.408107Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1609776890.4101086\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "print (time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T16:14:56.477574Z",
     "start_time": "2021-01-04T16:14:56.463091Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jan  4 21:44:56 2021\n"
     ]
    }
   ],
   "source": [
    "#Time class and usage\n",
    "import time;  # This is required to include time module.\n",
    "print (time.asctime( time.localtime(time.time()) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Spark Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T16:15:11.165905Z",
     "start_time": "2021-01-04T16:15:11.145443Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=PySparkShell>\n"
     ]
    }
   ],
   "source": [
    "# Import SparkContext class\n",
    "from pyspark import SparkContext\n",
    "# Create or use existing sparkcontext\n",
    "sc = SparkContext.getOrCreate()\n",
    "print (sc)\n",
    "#local* : no of workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T16:15:21.685501Z",
     "start_time": "2021-01-04T16:15:21.666151Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=PySparkShell>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method SparkContext.stop of <SparkContext master=local[*] appName=PySparkShell>>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stop spark context \n",
    "#from pyspark import SparkContext\n",
    "#sc = SparkContext.getOrCreate()\n",
    "print (sc)\n",
    "sc.stop # usually didnt stop better restat kernal from GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T16:15:26.783591Z",
     "start_time": "2021-01-04T16:15:26.763129Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.1\n"
     ]
    }
   ],
   "source": [
    "# Find spark version\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "print (sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T16:15:50.838552Z",
     "start_time": "2021-01-04T16:15:50.809125Z"
    }
   },
   "outputs": [],
   "source": [
    "#Import SparkConf\n",
    "from pyspark.conf import SparkConf\n",
    "# Create or use existing Spark session\n",
    "ss = SparkSession.builder.config(conf=SparkConf())\n",
    "#SparkSession provides a single point of entry to interact with underlying Spark functionality \n",
    "#A SparkSession can be used create DataFrame, register DataFrame as tables, \n",
    "#execute SQL over tables, cache tables, and read parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T16:16:03.982039Z",
     "start_time": "2021-01-04T16:16:03.952119Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession.Builder object at 0x0000027A0ABDD280>\n"
     ]
    }
   ],
   "source": [
    "#Import SparkConf\n",
    "from pyspark.conf import SparkConf\n",
    "# Create or use existing Spark session\n",
    "ss = SparkSession.builder.config(conf=SparkConf())\n",
    "# View Spark session object\n",
    "print (ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T16:16:19.733012Z",
     "start_time": "2021-01-04T16:16:19.698103Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession.Builder object at 0x0000027A0ABDD280>\n"
     ]
    }
   ],
   "source": [
    "#Ignore, another way to create session\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.types import *\n",
    "spark = SparkSession.builder.config(conf=SparkConf())\n",
    "print (spark)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create RDD's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plain RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T16:17:02.282868Z",
     "start_time": "2021-01-04T16:16:55.207016Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[7] at RDD at PythonRDD.scala:53\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating Plain RDD\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "#Initialize RDD named array\n",
    "array= sc.parallelize(range(10))\n",
    "print (array)\n",
    "array.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T16:17:49.332140Z",
     "start_time": "2021-01-04T16:17:43.114872Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating Plain RDD\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "#Initialize RDD named array\n",
    "array= sc.parallelize(range(10))\n",
    "#print (array)\n",
    "array.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T16:17:52.238093Z",
     "start_time": "2021-01-04T16:17:52.137360Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create data in Plain RDD\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "#--Spark version 2.4 and python 3.8 issue\n",
    "#array= sc.parallelize(    range(10))\n",
    "array= sc.parallelize(list(range(10)))\n",
    "# Print data in RDD array\n",
    "#array=str(array)\n",
    "#print (array)\n",
    "array.collect()\n",
    "#type(array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Key value RDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T16:18:25.600209Z",
     "start_time": "2021-01-04T16:18:25.519912Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "#kv= sc.parallelize(range(4))\n",
    "kv= sc.parallelize(list(range(4)))\n",
    "kv.collect()\n",
    "#kv.map(lambda x: (x,x*x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T16:22:09.430606Z",
     "start_time": "2021-01-04T16:22:03.265015Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0), (1, 1), (2, 4), (3, 9)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "kv= sc.parallelize(list(range(4)))\n",
    "#kv= sc.parallelize(list(range(4)))\n",
    "#kv.collect()\n",
    "kv.map(lambda x: (x,x*x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T16:22:13.639656Z",
     "start_time": "2021-01-04T16:22:13.605228Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "kv= sc.parallelize([(0, 0), (1, 1), (2, 4), (3, 9)])\n",
    "#kv.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T16:22:16.130453Z",
     "start_time": "2021-01-04T16:22:16.043175Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0), (1, 1), (2, 4), (3, 9)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "kv= sc.parallelize([(0, 0), (1, 1), (2, 4), (3, 9)])\n",
    "kv.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T16:22:21.236387Z",
     "start_time": "2021-01-04T16:22:21.157597Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0), (1, 1), (2, 4), (3, 9)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "kv= sc.parallelize([(0, 0), (1, 1), (2, 4), (3, 9)])\n",
    "kv.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD of Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T16:23:02.199258Z",
     "start_time": "2021-01-04T16:23:02.157838Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[26] at readRDDFromFile at PythonRDD.scala:262\n"
     ]
    }
   ],
   "source": [
    "# Create Plain RDD of Strings\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "array= sc.parallelize(['Movie \t Shakespeare in Love',\n",
    "'Directed by\tJohn Madden','Produced by\t David Parfitt',\n",
    "'Written by\t Marc Norman','Music by\tStephen Warbeck'])\n",
    "array.collect()\n",
    "print(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T16:32:10.379289Z",
     "start_time": "2021-01-04T16:31:48.937551Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 3),\n",
       " ('Had', 2),\n",
       " ('Little', 4),\n",
       " ('Mary', 1),\n",
       " ('fleece', 7),\n",
       " ('lamb', 5),\n",
       " ('was', 8),\n",
       " ('white', 9),\n",
       " ('whose', 6)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp2 = [('Mary', 1), ('Had', 2), ('A', 3), ('Little', 4), ('lamb', 5)]\n",
    "tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])\n",
    "sc.parallelize(tmp2).sortByKey(True, 3).collect()\n",
    "# .map(lambda x : x.lower())\n",
    "#[('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5),...('white', 9), ('whose', 6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T16:39:44.176014Z",
     "start_time": "2021-01-04T16:39:43.431059Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'CDE'),\n",
       " ('fleece', 7),\n",
       " ('had', 'B'),\n",
       " ('lamb', 5),\n",
       " ('little', 'F'),\n",
       " ('Mary', 'a'),\n",
       " ('was', 8),\n",
       " ('white', 9),\n",
       " ('whose', 6)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp2 = [('Mary', \"a\"), ('had', \"B\"), ('a', \"CDE\"), ('little', \"F\"), ('lamb', 5)]\n",
    "tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])\n",
    "#sortByKey(Asc=True, partition, keyfunc=lambda )\n",
    "sc.parallelize(tmp2).sortByKey(True, 1, keyfunc=lambda k: k.lower()).collect()\n",
    "#[('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5),...('white', 9), ('whose', 6)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD from Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T16:44:01.700646Z",
     "start_time": "2021-01-04T16:44:00.955100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BDSN software installation',\n",
       " '',\n",
       " 'https://drive.google.com/drive/folders/1e-x1mI6mfNnl1v3qZ_jgwfdauo39exxv',\n",
       " '',\n",
       " 'Gow-0.8.0.exe']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create RDD from text file data\n",
    "sc = SparkContext.getOrCreate()\n",
    "#data= sc.textFile(\"D:/E_drive/perseonal/Training/Spark/Data/strings.txt\")\n",
    "data= sc.textFile(\"../My exercises/software installation.txt\")\n",
    "data.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T16:45:14.318945Z",
     "start_time": "2021-01-04T16:45:14.196685Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BDSN software installation',\n",
       " '',\n",
       " 'https://drive.google.com/drive/folders/1e-x1mI6mfNnl1v3qZ_jgwfdauo39exxv',\n",
       " '',\n",
       " 'Gow-0.8.0.exe',\n",
       " 'jre-8u271-windows-x64.exe',\n",
       " 'spark-2.2.0-bin-hadoop2.7.zip',\n",
       " 'winutils.exe',\n",
       " '',\n",
       " 'setx SPARK_HOME C:\\\\sparkinstall\\\\spark-2.2.0-bin-hadoop2.7',\n",
       " 'setx HADOOP_HOME C:\\\\sparkinstall\\\\spark-2.2.0-bin-hadoop2.7',\n",
       " 'setx PYSPARK_DRIVER_PYTHON ipython',\n",
       " 'setx PYSPARK_DRIVER_PYTHON_OPTS notebook',\n",
       " '',\n",
       " 'spark-submit --version',\n",
       " '',\n",
       " 'C:\\\\Users\\\\Goutham-ROG\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39',\n",
       " '',\n",
       " 'C:\\\\sparkinstall\\\\spark-2.2.0-bin-hadoop2.7\\\\bin',\n",
       " '',\n",
       " 'pip install jupyter',\n",
       " '',\n",
       " '',\n",
       " 'goto ',\n",
       " 'C:\\\\sparkinstall\\\\spark-2.2.0-bin-hadoop2.7\\\\bin\\\\Spark Notebooks']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create RDD from text file data\n",
    "sc = SparkContext.getOrCreate()\n",
    "#data= sc.textFile(\"D:/E_drive/perseonal/Training/Spark/Data/strings.txt\")\n",
    "data= sc.textFile(\"../My exercises/software installation.txt\")\n",
    "#data.take(5)\n",
    "data.collect()\n",
    "# print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T16:45:06.964510Z",
     "start_time": "2021-01-04T16:45:05.338331Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create RDD from text file data\n",
    "sc = SparkContext.getOrCreate()\n",
    "#data= sc.textFile(\"D:/E_drive/perseonal/Training/Spark/Data/strings.txt\")\n",
    "data= sc.textFile(\"../My exercises/software installation.txt\")\n",
    "\n",
    "#print(data)\n",
    "\n",
    "#data.collect()\n",
    "#data.take(3)\n",
    "data.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Actions in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T16:45:24.755008Z",
     "start_time": "2021-01-04T16:45:24.722089Z"
    }
   },
   "source": [
    "a= [ 2, 3, 4, 5]\n",
    "sum=0\n",
    "for i in a:\n",
    "    sum =sum +(i*i)\n",
    "    \n",
    "print(sum)\n",
    "----------------------------\n",
    "i=2    --1sec\n",
    "sum = 0+(2*2) => 4 --2sec\n",
    "i=3\n",
    "Sum =4+ (3*3) ==> 13 ---3sec\n",
    "i=4\n",
    "sum = 13+4*4 ==> 29   --4sec\n",
    "i =5\n",
    "sum =29+(5*5) ==> 54 --5sec\n",
    "\n",
    "Mapping :  squaring each value of list\n",
    "----> 2*2    , 3*3 , 4*4 , 5*5\n",
    "\n",
    "[4,9,16,25]\n",
    "--> adding all elements of list   ---reduce\n",
    "4+9 , 16+25     --- 1 sec\n",
    "13 + 41   -- 2sec\n",
    "----> 54\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View data in Plain RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T16:45:59.408209Z",
     "start_time": "2021-01-04T16:45:53.150012Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View data in Plain RDD\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "array= sc.parallelize(range(10))\n",
    "# Print data in RDD array\n",
    "array.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Plain RDD of Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T16:46:01.012966Z",
     "start_time": "2021-01-04T16:46:00.973072Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Movie \\t Shakespeare in Love',\n",
       " 'Directed by\\tJohn Madden',\n",
       " 'Produced by\\t David Parfitt',\n",
       " 'Written by\\t Marc Norman',\n",
       " 'Music by\\tStephen Warbeck']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View Plain RDD of Strings\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "string= sc.parallelize(['Movie \t Shakespeare in Love',\n",
    "'Directed by\tJohn Madden', 'Produced by\t David Parfitt',\n",
    "'Written by\t Marc Norman', 'Music by\tStephen Warbeck'])\n",
    "string.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View RDD row count from text file data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T16:46:17.175417Z",
     "start_time": "2021-01-04T16:46:15.589094Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View RDD count from text file data\n",
    "sc = SparkContext.getOrCreate()\n",
    "#data= sc.textFile(\"D:/E_drive/perseonal/Training/Spark/Data/strings.txt\")\n",
    "data= sc.textFile(\"../My exercises/software installation.txt\")\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T16:47:31.253347Z",
     "start_time": "2021-01-04T16:47:31.145139Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BDSN software installation',\n",
       " '',\n",
       " 'https://drive.google.com/drive/folders/1e-x1mI6mfNnl1v3qZ_jgwfdauo39exxv',\n",
       " '',\n",
       " 'Gow-0.8.0.exe',\n",
       " 'jre-8u271-windows-x64.exe',\n",
       " 'spark-2.2.0-bin-hadoop2.7.zip',\n",
       " 'winutils.exe',\n",
       " '',\n",
       " 'setx SPARK_HOME C:\\\\sparkinstall\\\\spark-2.2.0-bin-hadoop2.7',\n",
       " 'setx HADOOP_HOME C:\\\\sparkinstall\\\\spark-2.2.0-bin-hadoop2.7',\n",
       " 'setx PYSPARK_DRIVER_PYTHON ipython',\n",
       " 'setx PYSPARK_DRIVER_PYTHON_OPTS notebook',\n",
       " '',\n",
       " 'spark-submit --version',\n",
       " '',\n",
       " 'C:\\\\Users\\\\Goutham-ROG\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39',\n",
       " '',\n",
       " 'C:\\\\sparkinstall\\\\spark-2.2.0-bin-hadoop2.7\\\\bin',\n",
       " '',\n",
       " 'pip install jupyter',\n",
       " '',\n",
       " '',\n",
       " 'goto ',\n",
       " 'C:\\\\sparkinstall\\\\spark-2.2.0-bin-hadoop2.7\\\\bin\\\\Spark Notebooks']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View RDD content from text file data\n",
    "sc = SparkContext.getOrCreate()\n",
    "#data= sc.textFile(\"D:/E_drive/perseonal/Training/Spark/Data/strings.txt\")\n",
    "data= sc.textFile(\"../My exercises/software installation.txt\")\n",
    "data.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Random 5 rows inRDD content from text file data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T16:47:36.775354Z",
     "start_time": "2021-01-04T16:47:36.022968Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BDSN software installation',\n",
       " '',\n",
       " 'https://drive.google.com/drive/folders/1e-x1mI6mfNnl1v3qZ_jgwfdauo39exxv',\n",
       " '',\n",
       " 'Gow-0.8.0.exe']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View 5 rows inRDD content from text file data\n",
    "sc = SparkContext.getOrCreate()\n",
    "#data= sc.textFile(\"D:/E_drive/perseonal/Training/Spark/Data/strings.txt\")\n",
    "data= sc.textFile(\"../My exercises/software installation.txt\")\n",
    "data.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting values in RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T16:47:55.457804Z",
     "start_time": "2021-01-04T16:47:49.242129Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting values in RDD\n",
    "a = range(100)  \n",
    "datarange = sc.parallelize(a)\n",
    "datarange.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Random 5 elements from RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T16:48:14.094562Z",
     "start_time": "2021-01-04T16:48:13.478128Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View 5 elements from RDD\n",
    "a = range(100)  \n",
    "datarange = sc.parallelize(a)\n",
    "datarange.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View ordered 5 elements from RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T16:50:08.649560Z",
     "start_time": "2021-01-04T16:49:56.438152Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 2, 3]\n",
      "[8, 7, 6, 5, 5]\n"
     ]
    }
   ],
   "source": [
    "# View 5 elements from RDD\n",
    "a = range(100)  \n",
    "RDD1 = sc.parallelize(a)\n",
    "RDD2 = sc.parallelize([2,5,3,6,7,2,0,1,5,8])\n",
    "#print (RDD1.top(5))\n",
    "#print (RDD2.top(5))\n",
    "print (RDD2.takeOrdered(5))\n",
    "print (RDD2.top(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating sample data randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T16:51:00.957128Z",
     "start_time": "2021-01-04T16:50:18.151222Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[3, 9, 3, 1, 0, 0, 6, 8, 3, 6, 9, 9, 2, 1, 0, 1, 2, 2, 4, 9]\n",
      "[5, 9, 3, 4, 6]\n",
      "[1, 5, 6, 0, 9, 4, 7, 2, 8, 3]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(0, 10))\n",
    "print(rdd.collect())\n",
    "print(rdd.takeSample(True, 20, 1))\n",
    "print(rdd.takeSample(False, 5, 2))\n",
    "print(rdd.takeSample(False, 15, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple action on RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T16:51:03.410753Z",
     "start_time": "2021-01-04T16:51:00.961120Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BDSN software installation',\n",
       " '',\n",
       " 'https://drive.google.com/drive/folders/1e-x1mI6mfNnl1v3qZ_jgwfdauo39exxv',\n",
       " '',\n",
       " 'Gow-0.8.0.exe',\n",
       " 'jre-8u271-windows-x64.exe',\n",
       " 'spark-2.2.0-bin-hadoop2.7.zip',\n",
       " 'winutils.exe',\n",
       " '',\n",
       " 'setx SPARK_HOME C:\\\\sparkinstall\\\\spark-2.2.0-bin-hadoop2.7',\n",
       " 'setx HADOOP_HOME C:\\\\sparkinstall\\\\spark-2.2.0-bin-hadoop2.7',\n",
       " 'setx PYSPARK_DRIVER_PYTHON ipython',\n",
       " 'setx PYSPARK_DRIVER_PYTHON_OPTS notebook',\n",
       " '',\n",
       " 'spark-submit --version',\n",
       " '',\n",
       " 'C:\\\\Users\\\\Goutham-ROG\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39',\n",
       " '',\n",
       " 'C:\\\\sparkinstall\\\\spark-2.2.0-bin-hadoop2.7\\\\bin',\n",
       " '',\n",
       " 'pip install jupyter',\n",
       " '',\n",
       " '',\n",
       " 'goto ',\n",
       " 'C:\\\\sparkinstall\\\\spark-2.2.0-bin-hadoop2.7\\\\bin\\\\Spark Notebooks']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "data= sc.textFile(\"../My exercises/software installation.txt\")\n",
    "data.count() # count of rows\n",
    "data.take(5) # Return top 5 rows\n",
    "data.collect() # Return complete file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addition of each element in RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T16:51:09.785381Z",
     "start_time": "2021-01-04T16:51:03.413746Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "Wall time: 6.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Addition of each element in RDD\n",
    "from time import time\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "array = sc.parallelize(range(10))\n",
    "#Combining result from RDD elements\n",
    "print(array.reduce(lambda x,y: x+y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T16:57:43.651347Z",
     "start_time": "2021-01-04T16:57:43.574031Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "a =[0,1,2,3,4,5,6,7]\n",
    "array = sc.parallelize(a)\n",
    "array.collect()\n",
    "#Combining result from RDD elements\n",
    "#print(array.reduce(lambda x,y: x+y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T16:57:53.253568Z",
     "start_time": "2021-01-04T16:57:47.355118Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "Wall time: 5.88 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Square of each element in RDD\n",
    "from time import time\n",
    "import time;from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "x= sc.parallelize(range(10))\n",
    "print(x.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is below python code faster than Spark code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T16:57:57.862448Z",
     "start_time": "2021-01-04T16:57:57.840021Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Square of each element in RDD\n",
    "from time import time\n",
    "import time;\n",
    "x= range(10)\n",
    "print(sum(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets increase the ammount of data, If you are on custer(multiple nodes) spark will take less otherwise depend on single node cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T16:58:17.989292Z",
     "start_time": "2021-01-04T16:58:11.971071Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499500\n",
      "Wall time: 6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Addition of each element in RDD\n",
    "from time import time\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "array = sc.parallelize(range(1000))\n",
    "#Combining result from RDD elements\n",
    "print(array.reduce(lambda x,y: x+y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T16:58:18.005249Z",
     "start_time": "2021-01-04T16:58:17.993281Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499500\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Square of each element in RDD\n",
    "from time import time\n",
    "import time;\n",
    "x= range(1000)\n",
    "print(sum(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find shortest string in String RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T16:58:50.694011Z",
     "start_time": "2021-01-04T16:58:44.792866Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find shortest string in RDD\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "# Creating List\n",
    "words = ['This','is','Algae','Code']\n",
    "#Converting list to RDD\n",
    "RDD = sc.parallelize(words)\n",
    "#Combining result from RDD elements\n",
    "RDD.reduce(lambda x,y: x if len(x)< len(y) else y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subtract of each element in RDD could go wrong in case of Multiple nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bad usage\n",
    "# Subtract of each element in RDD\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "array = sc.parallelize([1,3,5,2])\n",
    "#Combining result from RDD elements\n",
    "array.reduce(lambda x,y: x-y)\n",
    "#(1-3-5-2) | (1-3-(5-2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# last word in lexiographic order among longest word in RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coool'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# last word in lexiographic order among longest word in RDD\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "# Creating List\n",
    "words = ['This','is','Algae','Code','That','was','so','Coool']\n",
    "#Converting list to RDD\n",
    "RDD = sc.parallelize(words)\n",
    "#Combining result from RDD elements\n",
    "def largerThan(x,y):\n",
    "    if len(x)>len(y): return(x)\n",
    "    elif len(x)<len(y): return (y)\n",
    "    else: \n",
    "        if x>y:return (x) \n",
    "        else : return (y)\n",
    "RDD.reduce(largerThan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Algae', 'Code', 'Coool', 'That', 'This', 'is', 'so', 'was']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Creating List\n",
    "words = ['This','is','Algae','Code','That','was','so','Coool']\n",
    "words.sort()\n",
    "print (words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\n"
     ]
    }
   ],
   "source": [
    "def largerThan(words,word):\n",
    "    for x in words:\n",
    "        for y in word:\n",
    "            if len(x)>len(y): return(x)\n",
    "            elif len(x)<len(y): return (y)\n",
    "            else: \n",
    "                if x>y:return (x) \n",
    "                else : return (y)\n",
    "                \n",
    "words = ['This','is','Algae','Code','That','was','so','Coool']\n",
    "word = ['is','Algae','Code','That','was','so','Coool']\n",
    "print(largerThan(words,word))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 3, 4, 6]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc.parallelize([0, 2, 3, 4, 6], 5).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0], [2], [3], [4], [6]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "# Glom will covert all element of list into list itself\n",
    "sc.parallelize([0, 2, 3, 4, 6], 5).glom().collect()\n",
    "#[[0], [2], [3], [4], [6]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc.parallelize([1, 2, 3, 4, 5, 6]).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "word = ['is','Algae','Code','That','was','so','Coool']\n",
    "sc.parallelize(word).first()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 9, 3, 1, 0, 0, 6, 7, 4, 6, 8, 9, 3, 2, 0, 2, 3, 2, 5, 8]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#takeSample(withReplacement, num, seed=None)\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "rdd = sc.parallelize(range(0, 10))\n",
    "rdd.takeSample(True, 20, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:14:25.441039Z",
     "start_time": "2021-01-04T17:14:13.295037Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 8, 9, 7, 5, 3, 0, 4, 1, 2]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "rdd = sc.parallelize(range(0, 10))\n",
    "rdd.takeSample(False, 20, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 2]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "rdd = sc.parallelize(range(0, 10))\n",
    "rdd.takeSample(False, 2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 2), ('a', 1), ('1', 3), ('d', 4), ('2', 5)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
    "a=sc.parallelize(tmp)\n",
    "a.reduceByKey(lambda x,y:x+y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
    "sc.parallelize(tmp).sortByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1', 3)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
    "sc.parallelize(tmp).sortByKey().first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:15:45.142376Z",
     "start_time": "2021-01-04T17:15:44.488105Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sortByKey(ascending=True, numPartitions=None, keyfunc=<function <lambda>>)\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
    "sc.parallelize(tmp).sortByKey(True, 1).collect()\n",
    "#[('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:17:20.210454Z",
     "start_time": "2021-01-04T17:17:00.166128Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sortByKey(ascending=True, numPartitions=None, keyfunc=<function <lambda>>)\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
    "sc.parallelize(tmp).sortByKey(True, 2).collect()\n",
    "#[('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 3),\n",
       " ('fleece', 7),\n",
       " ('had', 2),\n",
       " ('lamb', 5),\n",
       " ('little', 4),\n",
       " ('Mary', 1),\n",
       " ('was', 8),\n",
       " ('white', 9),\n",
       " ('whose', 6)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sortByKey(ascending=True, numPartitions=None, keyfunc=<function <lambda>>)\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]\n",
    "tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])\n",
    "sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()\n",
    "#[('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5),...('white', 9), ('whose', 6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]\n",
    "#tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])\n",
    "a = sc.parallelize(tmp2).sortByKey(True, 10, keyfunc=lambda k: k.lower())\n",
    "a.getNumPartitions() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(sc.parallelize([3, 1, 2, 3]).distinct().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc.parallelize([3, 1, 2, 3]).distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "#sort by Key\n",
    "tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
    "sc.parallelize(tmp).sortBy(lambda x: x[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "#sort by Value\n",
    "tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
    "sc.parallelize(tmp).sortBy(lambda x: x[1]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-5, -3, -2, -1, 1, 2, 3, 4, 4, 5]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "#sort the output\n",
    "a =[1,-1,2,-2,3,-3,4,4,5,-5]\n",
    "sc.parallelize(a).sortBy(lambda x: x ).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort by Element\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
    "sc.parallelize(tmp).sortBy(lambda x: x[1]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3], [4, 5, 3, 2]]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Divide data into partition\n",
    "rdd = sc.parallelize([1, 2, 3, 4,5,3,2], 2)\n",
    "rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation Commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Square of each element in RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Square of each element in RDD\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "array = sc.parallelize(range(10))\n",
    "#applying operation on each element of RDD\n",
    "array.map(lambda x: x*x).collect()\n",
    "#print(array.map(lambda x: x*x).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify time take generating square of each element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:21:45.587351Z",
     "start_time": "2021-01-04T17:21:39.257086Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.32 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Square of each element in RDD\n",
    "from time import time\n",
    "import time;\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "array = sc.parallelize(range(10))\n",
    "#applying operation on each element of RDD\n",
    "array.map(lambda x: x*x).collect()\n",
    "#print(array.map(lambda x: x*x).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# validating RDDs are DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:23:08.464815Z",
     "start_time": "2021-01-04T17:23:02.302557Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.14 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Square of each element in RDD\n",
    "from time import time\n",
    "import time;\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "array = sc.parallelize(list(range(10)))\n",
    "#applying operation on each element of RDD\n",
    "array.map(lambda x: x*x).collect()\n",
    "# print(array.map(lambda x: x*x).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:22:19.393191Z",
     "start_time": "2021-01-04T17:22:13.469131Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jan  4 22:52:13 2021\n",
      "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n",
      "Mon Jan  4 22:52:19 2021\n",
      "Wall time: 5.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Square of each element in RDD\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "array = sc.parallelize(range(10))\n",
    "#applying operation on each element of RDD\n",
    "print (time.asctime( time.localtime(time.time()) ))\n",
    "print(array.map(lambda x: x*x).collect())\n",
    "print (time.asctime( time.localtime(time.time()) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 9], [4, 16], [5, 25]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import time\n",
    "import time;\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc.parallelize([3,4,5]).map(lambda x: [x,  x*x]).collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Stack Overflow  Questions Developer Jobs'],\n",
       " ['Tags Users Search Log In Sign Up']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "array = sc.parallelize(range(10))\n",
    "data= sc.textFile(\"D:/E_drive/perseonal/Training/Spark/Data/strings.txt\")\n",
    "#Pick each line as element of RDD\n",
    "b = data.map(lambda line : line.split('/t'))\n",
    "b.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 9, 4, 16, 5, 25]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import time\n",
    "import time;\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc.parallelize([3,4,5]).flatMap(lambda x: [x, x*x]).collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Stack', 'Overflow']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "array = sc.parallelize(range(10))\n",
    "data= sc.textFile(\"D:/Perseonal/MyTrainings/Spark Training/Spark-Notes/Lab/strings.txt\")\n",
    "# Pick each word as element of RDD\n",
    "a = data.flatMap(lambda line : line.split(\" \"))\n",
    "a.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Stack', 'Overflow', '', 'Questions', 'Developer', 'Jobs'],\n",
       " ['Tags', 'Users', 'Search', 'Log', 'In', 'Sign', 'Up']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "data= sc.textFile(\"D:/Perseonal/MyTrainings/Spark Training/Spark-Notes/Lab/strings.txt\")\n",
    "#Pick each line as element of RDD\n",
    "b = data.map(lambda line : line.split(' '))\n",
    "b.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Stack Overflow  Questions Developer Jobs',\n",
       " 'Tags Users Search Log In Sign Up']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "data= sc.textFile(\"D:/E_drive/perseonal/Training/Spark/Data/strings.txt\")\n",
    "#Pick each line as element of RDD\n",
    "b = data.flatMap(lambda line : line.split('/t'))\n",
    "b.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Filter Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 7, 8, 9]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "a =sc.parallelize (range(10))\n",
    "b= a.filter(lambda x: x>5)\n",
    "b.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EFG']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "a =sc.parallelize (['A','BC','D','EFG'])\n",
    "b = a.filter(lambda x: len(x) > 2)\n",
    "b.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1--------------------------------------------',\n",
       " '2-----------------------------------',\n",
       " 'self.mapPartitions(func).collect()']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "array = sc.parallelize(range(10))\n",
    "data= sc.textFile(\"D:/E_drive/perseonal/Training/Spark/Data/strings.txt\")\n",
    "# Pick each word as element of RDD\n",
    "a = data.flatMap(lambda line : line.split(\" \"))\n",
    "b = a.filter(lambda x: len(x) > 30)\n",
    "b.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my', 'my']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "data= sc.textFile(\"D:/E_drive/perseonal/Training/Spark/Data/strings.txt\")\n",
    "a= data.flatMap(lambda lines: lines.split(\" \")).filter(lambda value: value==\"my\")\n",
    "a.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Stack', 'Overflow']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "data= sc.textFile(\"D:/E_drive/perseonal/Training/Spark/Data/strings.txt\")\n",
    "b= data.flatMap(lambda lines: lines.split(\" \")).filter(lambda value: value!=\"my\")\n",
    "b.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 6, 8]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "a =sc.parallelize (range(10))\n",
    "b= a.map(lambda x: x if x%2==0 else '' ).filter(lambda x: x!='')\n",
    "b.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 1, 8, 3]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([5, 1, 8, 3])\n",
    "rdd2 = sc.parallelize(range(5))\n",
    "rdd2.union(rdd1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'BC', 'ZSK', 'GH', 'A', 'BC', 'DEF', 'GH']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize(['A', 'BC', 'DEF' ,'GH'])\n",
    "rdd2 = sc.parallelize(['A', 'UC', 'ZSK' ,'GH'])\n",
    "rdd2.union(rdd1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GH', 'A']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize(['A', 'BC', 'DEF' ,'GH'])\n",
    "rdd2 = sc.parallelize(['A', 'UC', 'ZSK' ,'GH'])\n",
    "rdd2.intersection(rdd1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([5, 1, 8, 3])\n",
    "rdd2 = sc.parallelize(range(5))\n",
    "rdd2.intersection(rdd1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (1, 2)), ('a', (1, 3))]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inner Join\n",
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\", 2), (\"a\", 3)])\n",
    "x.join(y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (1, 2)), ('a', (1, 3)), ('b', (4, None))]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\", 2), (\"a\", 3)])\n",
    "x.leftOuterJoin(y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (1, 2)), ('a', (1, 3))]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\", 2), (\"a\", 3)])\n",
    "x.rightOuterJoin(y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Stack', 'Overflow']"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "def myfun(a):\n",
    "    word = a.split(' ')\n",
    "    return word\n",
    "\n",
    "a = sc.textFile(\"D:/E_drive/perseonal/Training/Spark/Data/strings.txt\")\n",
    "b = a.flatMap(lambda line: myfun(line))\n",
    "b.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KEY Value RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{1: 10}, {2: 20}, {3: 30}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "data = sc.parallelize([ {1: 10},{2:20},{3:30}])\n",
    "data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{1, 10}, {2, 20}, {3, 30}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "data = sc.parallelize([ {1, 10},{2,20},{3,30}])\n",
    "data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, {'Dept': 'Edu', 'name': 'Saurabh'}),\n",
       " (2, {'Dept': 'Admin', 'name': 'Kunal'})]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "data = sc.parallelize(\n",
    "    [(1, {'name': 'Saurabh', 'Dept': 'Edu'}),\n",
    "     ( 2,{'name': 'Kunal',  'Dept':'Admin'})])\n",
    "#data.collect()\n",
    "# Key and key value pair as value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Saurabh': 100}, {'Kual': 200}, {'Saurabh': 200}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "data = sc.parallelize([ {'Saurabh': 100},{'Kual':200},{'Saurabh':200}])\n",
    "data.collect()\n",
    "# Here key is identifier but it doesnt need to be unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation | Key value Pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0), (1, 1), (2, 4), (3, 9)]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform to Key value Pair\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "data = sc.parallelize(range(4))\n",
    "final = data.map(lambda x: (x,x*x))\n",
    "final.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Actions on Key Value Pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 5), (2, 7)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summation of Key value Pair on the basis of Key\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "data = sc.parallelize([(1,2),(1,3),(2,4),(2,3)])\n",
    "final = data.reduceByKey(lambda x,y: (x+y))\n",
    "final.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, -1), (2, 1)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subtraction of Key value Pair on the basis of Key\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "data = sc.parallelize([(1,2),(1,3),(2,4),(2,3)])\n",
    "final = data.reduceByKey(lambda x,y: (x-y))\n",
    "final.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', '1'), ('a', '1145')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "data = [(\"a\", 1), (\"b\", 1), (\"a\", 1),(\"a\", 45)]\n",
    "x = sc.parallelize(data)\n",
    "def add(a, b): return a + str(b)\n",
    "#sorted(x.combineByKey(str, add, add).collect())\n",
    "x.combineByKey(str, add, add).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', (15.0, 3)), ('B', (30.0, 2)), ('Z', (28.0, 4))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "data = [\n",
    "        ('A', 2.), ('A', 4.), ('A', 9.), \n",
    "        ('B', 10.), ('B', 20.), \n",
    "        ('Z', 3.), ('Z', 5.), ('Z', 8.), ('Z', 12.) \n",
    "       ]\n",
    "\n",
    "rdd = sc.parallelize( data )\n",
    "sumCount = rdd.combineByKey(lambda value: (value, 1),\n",
    "                            lambda x, value: (x[0] + value, x[1] + 1),\n",
    "                            lambda x, y: (x[0] + y[0], x[1] + y[1])\n",
    "                           )\n",
    "sumCount.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:31:34.487752Z",
     "start_time": "2021-01-04T17:30:56.825082Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('b', 4), ('b', 5)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subtract between 2 arrays\n",
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 3)])\n",
    "y = sc.parallelize([(\"a\", 3), (\"c\", None),(\"b\", 2)])\n",
    "#x.collect()\n",
    "#y.collect()\n",
    "sorted(x.subtract(y).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common functions on  RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:31:40.788972Z",
     "start_time": "2021-01-04T17:31:34.491742Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add elements of array\n",
    "x= sc.parallelize([1.0, 2.0, 3.0])\n",
    "x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T17:32:05.097475Z",
     "start_time": "2021-01-04T17:31:59.014936Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.816496580927726"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find standard deviation of array values\n",
    "x= sc.parallelize([1, 2, 3])\n",
    "x.stdev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find variation of array values\n",
    "x= sc.parallelize([1.0, 2.0, 3.0])\n",
    "x.variance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(count: 4, mean: 2.5, stdev: 1.11803398875, max: 4.0, min: 1.0)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find stats array values\n",
    "x= sc.parallelize([1.0, 2.0, 3.0,4.0])\n",
    "x.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting 1 -s array to 2-d array by keeping y constant in (x,y)\n",
    "sc = SparkContext.getOrCreate()\n",
    "a = range(5) \n",
    "b = sc.parallelize (a)\n",
    "increment = b.map(lambda x: (x,1))\n",
    "increment.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0), (1, 1), (1, 2), (1, 3), (1, 4)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting 1 -s array to 2-d array by keeping X constant in (x,y)\n",
    "a = range(5) \n",
    "b = sc.parallelize (a)\n",
    "increment = b.map(lambda x: (1,x))\n",
    "increment.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 2), (1, 3), (2, 4), (3, 5), (4, 6)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting 1 -d array to 2-d array by keeping X constant in (x,y) and increment each value of y by 2\n",
    "a = range(5) \n",
    "b = sc.parallelize (a)\n",
    "increment = b.map(lambda x: (x,x+2))\n",
    "increment.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Abhishek', 8), ('Alok', 4), ('Vishnu', 6)]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract lenth of each element in array\n",
    "x = sc.parallelize([\"Abhishek\", \"Alok\", \"Vishnu\"])\n",
    "y = x.map(lambda x: (x,len(x)))\n",
    "y.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abhishek 8\n",
      "Alok 4\n",
      "Vishnu 6\n",
      "Aliya 5\n",
      "Ram 3\n",
      "Ashwini 7\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Only python, in multiple nodes spark will be faster\n",
    "x = ([\"Abhishek\", \"Alok\", \"Vishnu\",  \"Aliya\", \"Ram\" , \"Ashwini\"])\n",
    "for i in x:\n",
    "    y = len(i)\n",
    "    print (i,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(count: 6, mean: 5.5, stdev: 1.70782512766, max: 8.0, min: 3.0)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create stats of lenth of each element in array\n",
    "x = sc.parallelize([\"Abhishek\", \"Alok\", \"Vishnu\",  \"Aliya\", \"Ram\" , \"Ashwini\"])\n",
    "y=  x.map(lambda y:(len(y)))\n",
    "y.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(count: 237, mean: 34.400843881856545, stdev: 68.5711592519, max: 635.0, min: 0.0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create stats of lenth of each element of data in text file\n",
    "data= sc.textFile(\"D:/E_drive/perseonal/Training/Spark/Data/strings.txt\")\n",
    "y=  data.map(lambda y:(len(y)))\n",
    "y.collect()\n",
    "y.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Operations on Text file data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['YEAR', 'GoogleKnowlege_Occupation', 'Show', 'Group', 'Raw_Guest_List'],\n",
       " ['1999', 'actor', '01-11-1999', 'Acting', 'Michael J. Fox'],\n",
       " ['1999', 'Comedian', '01-12-1999', 'Comedy', 'Sandra Bernhard'],\n",
       " ['1999', 'television actress', '1/13/99', 'Acting', 'Tracey Ullman'],\n",
       " ['1999', 'film actress', '1/14/99', 'Acting', 'Gillian Anderson']]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract top 5 rows from text file (data is in tabular format)\n",
    "sc = SparkContext.getOrCreate()\n",
    "raw_data = sc.textFile(\"./Data/OCP_data.tsv\")\n",
    "raw_data.take(1)\n",
    "daily_show = raw_data.map(lambda line: line.split('\\t'))\n",
    "daily_show.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('YEAR', 1),\n",
       " ('2002', 159),\n",
       " ('2003', 166),\n",
       " ('2004', 164),\n",
       " ('2007', 141),\n",
       " ('2010', 165),\n",
       " ('2011', 163),\n",
       " ('2012', 164),\n",
       " ('2013', 166),\n",
       " ('2014', 163),\n",
       " ('2015', 100),\n",
       " ('1999', 166),\n",
       " ('2000', 169),\n",
       " ('2001', 157),\n",
       " ('2005', 162),\n",
       " ('2006', 161),\n",
       " ('2008', 164),\n",
       " ('2009', 163)]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No. of Shows per Year\n",
    "sc = SparkContext.getOrCreate()\n",
    "raw_data = sc.textFile(\"./Data/OCP_data.tsv\")\n",
    "daily_show = raw_data.map(lambda line: line.split('\\t'))\n",
    "tally = daily_show.map(lambda x: (x[0], 1)).reduceByKey(lambda x,y: x+y)\n",
    "tally.take(tally.count())\n",
    "#tally.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('YEAR GoogleKnowlege_Occupation', 1),\n",
       " ('1999 actor', 53),\n",
       " ('1999 Comedian', 5),\n",
       " ('1999 television actress', 5),\n",
       " ('1999 Singer-lyricist', 1),\n",
       " ('1999 actress', 42),\n",
       " ('1999 Singer-songwriter', 2),\n",
       " ('1999 Comic', 1),\n",
       " ('1999 rock band', 2),\n",
       " ('1999 musician', 2)]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How many show per group of year and occupation\n",
    "sc = SparkContext.getOrCreate()\n",
    "raw_data = sc.textFile(\"./Data/OCP_data.tsv\")\n",
    "daily_show = raw_data.map(lambda line: line.split('\\t'))\n",
    "tally = daily_show.map(lambda x: (x[0]+' ' +x[1], 1))\n",
    "tally.reduceByKey(lambda x,y: x+y).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Exercise  on Automobil data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['symboling normalized-losses make fuel-type aspiration num-of-doors body-style drive-wheels engine-location wheel-base length width height curb-weight engine-type num-of-cylinders engine-size fuel-system bore stroke compression-ratio horsepower peak-rpm city-mpg highway-mpg price',\n",
       " '3 ? alfa-romero gas std two convertible rwd front 88.6 168.8 64.1 48.8 2548 dohc four 130 mpfi 3.47 2.68 9 111 5000 21 27 13495']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "raw_data = sc.textFile(\"./Data/automobile.txt\")\n",
    "raw_data.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['symboling',\n",
       " 'normalized-losses',\n",
       " 'make',\n",
       " 'fuel-type',\n",
       " 'aspiration',\n",
       " 'num-of-doors',\n",
       " 'body-style',\n",
       " 'drive-wheels',\n",
       " 'engine-location',\n",
       " 'wheel-base',\n",
       " 'length',\n",
       " 'width',\n",
       " 'height',\n",
       " 'curb-weight',\n",
       " 'engine-type',\n",
       " 'num-of-cylinders',\n",
       " 'engine-size',\n",
       " 'fuel-system',\n",
       " 'bore',\n",
       " 'stroke',\n",
       " 'compression-ratio',\n",
       " 'horsepower',\n",
       " 'peak-rpm',\n",
       " 'city-mpg',\n",
       " 'highway-mpg',\n",
       " 'price',\n",
       " '3',\n",
       " '?',\n",
       " 'alfa-romero',\n",
       " 'gas',\n",
       " 'std',\n",
       " 'two',\n",
       " 'convertible',\n",
       " 'rwd',\n",
       " 'front',\n",
       " '88.6',\n",
       " '168.8',\n",
       " '64.1',\n",
       " '48.8',\n",
       " '2548',\n",
       " 'dohc',\n",
       " 'four',\n",
       " '130',\n",
       " 'mpfi',\n",
       " '3.47',\n",
       " '2.68',\n",
       " '9',\n",
       " '111',\n",
       " '5000',\n",
       " '21',\n",
       " '27',\n",
       " '13495',\n",
       " '3',\n",
       " '?',\n",
       " 'alfa-romero',\n",
       " 'gas',\n",
       " 'std',\n",
       " 'two',\n",
       " 'convertible',\n",
       " 'rwd',\n",
       " 'front',\n",
       " '88.6',\n",
       " '168.8',\n",
       " '64.1',\n",
       " '48.8',\n",
       " '2548',\n",
       " 'dohc',\n",
       " 'four',\n",
       " '130',\n",
       " 'mpfi',\n",
       " '3.47',\n",
       " '2.68',\n",
       " '9',\n",
       " '111',\n",
       " '5000',\n",
       " '21',\n",
       " '27',\n",
       " '16500',\n",
       " '1',\n",
       " '?',\n",
       " 'alfa-romero',\n",
       " 'gas',\n",
       " 'std',\n",
       " 'two',\n",
       " 'hatchback',\n",
       " 'rwd',\n",
       " 'front',\n",
       " '94.5',\n",
       " '171.2',\n",
       " '65.5',\n",
       " '52.4',\n",
       " '2823',\n",
       " 'ohcv',\n",
       " 'six',\n",
       " '152',\n",
       " 'mpfi',\n",
       " '2.68',\n",
       " '3.47',\n",
       " '9',\n",
       " '154']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract top 5 rows from text file (data is in tabular format)\n",
    "sc = SparkContext.getOrCreate()\n",
    "raw_data = sc.textFile(\"./Data/automobile.txt\")\n",
    "raw_data.take(1)\n",
    "daily_show = raw_data.flatMap(lambda line: line.split(' '))\n",
    "daily_show.take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many show per group of year and occupation\n",
    "sc = SparkContext.getOrCreate()\n",
    "raw_data = sc.textFile(\"./Data/automobile.txt\")\n",
    "daily_show = raw_data.map(lambda line: line.split('\\t'))\n",
    "tally = daily_show.map(lambda x: (x[0]+' ' +x[1], 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/C:/sparkinstall/spark-2.2.0-bin-hadoop2.7/bin/Code/Data/automobile.txt\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:194)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)\r\n\tat scala.Option.getOrElse(Option.scala:121)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:250)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)\r\n\tat scala.Option.getOrElse(Option.scala:121)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:250)\r\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:53)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)\r\n\tat scala.Option.getOrElse(Option.scala:121)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:250)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:458)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-ae30ad0f7d8a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtally\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\sparkinstall\\spark-2.2.0-bin-hadoop2.7\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    807\u001b[0m         \"\"\"\n\u001b[0;32m    808\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 809\u001b[1;33m             \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    810\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    811\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\sparkinstall\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\sparkinstall\\spark-2.2.0-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\sparkinstall\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/C:/sparkinstall/spark-2.2.0-bin-hadoop2.7/bin/Code/Data/automobile.txt\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:194)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)\r\n\tat scala.Option.getOrElse(Option.scala:121)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:250)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)\r\n\tat scala.Option.getOrElse(Option.scala:121)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:250)\r\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:53)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)\r\n\tat scala.Option.getOrElse(Option.scala:121)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:250)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:458)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "tally.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many show per group of year and occupation\n",
    "sc = SparkContext.getOrCreate()\n",
    "raw_data = sc.textFile(\"./Data/automobile.txt\")\n",
    "daily_show = raw_data.map(lambda line: line.split('\\t'))\n",
    "tally = daily_show.map(lambda x: (x[0]+' ' +x[1], 1))\n",
    "tally.reduceByKey(lambda x,y: x+y).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[91, 92, 93]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(range(100), 100).filter(lambda x: x > 90).take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('YEAR', 'GoogleKnowlege_Occupation'),\n",
       " ('1999', 'actor'),\n",
       " ('1999', 'Comedian'),\n",
       " ('1999', 'television actress'),\n",
       " ('1999', 'film actress')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In last update range works same as xrange\n",
    "sc = SparkContext.getOrCreate()\n",
    "raw_data = sc.textFile(\"./Data/OCP_data.tsv\")\n",
    "raw_data.take(1)\n",
    "daily_show = raw_data.map(lambda line: line.split('\\t'))\n",
    "K_data =daily_show.map(lambda k: (k[0],k[1]))\n",
    "#G_data =K_data.groupByKey().map(lambda k, j: (k,[x for x in j]))\n",
    "#G_data.take(5)\n",
    "K_data.take (5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 3, 4]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# provide values for respective keys\n",
    "sc = SparkContext.getOrCreate()\n",
    "a =sc.parallelize ([(1,3),(2,3),(1,3),(2,3),(1,4),(2,8)])\n",
    "a.lookup(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 4, 2: 1, 3: 4}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# provide dictionary intead tupple\n",
    "sc = SparkContext.getOrCreate()\n",
    "a =sc.parallelize ([(1,3),(2,3),(1,3),(2,3),(1,4),(2,8),(3,4),(2,1)])\n",
    "a.collectAsMap()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
