{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T03:08:09.034623Z",
     "start_time": "2020-12-21T03:08:09.020670Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=PySparkShell>\n"
     ]
    }
   ],
   "source": [
    "#Using these 3 lines we create the connection\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T03:08:11.126832Z",
     "start_time": "2020-12-21T03:08:11.122843Z"
    }
   },
   "outputs": [],
   "source": [
    "# to stops the connection -> sleep mode\n",
    "# sc.stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T03:08:17.082346Z",
     "start_time": "2020-12-21T03:08:12.332839Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating a plain RDD\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "#initialise RDD named array\n",
    "kv = sc.parallelize(range(10))\n",
    "kv.collect()\n",
    "# a.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T03:08:19.568044Z",
     "start_time": "2020-12-21T03:08:19.563057Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.1\n"
     ]
    }
   ],
   "source": [
    "print(sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T03:08:22.237786Z",
     "start_time": "2020-12-21T03:08:20.063824Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: py4j in c:\\users\\goutham-rog\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.10.9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.3.1; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the 'c:\\users\\goutham-rog\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install py4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T03:08:22.316587Z",
     "start_time": "2020-12-21T03:08:22.238784Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0), (1, 1), (2, 4), (3, 4)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kv = sc.parallelize([(0,0),(1,1),(2,4),(3,4)])\n",
    "kv.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T03:08:45.380998Z",
     "start_time": "2020-12-21T03:08:41.601101Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16, 25]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kv1 = sc.parallelize(range(6))\n",
    "kv1.map(lambda x: (x**2)).collect()\n",
    "#map function is used when we want to apply the fn(map) it o all the  df or list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reduce\n",
    "#we use reduce fn, if we want to colalte and tehn perform the operation like sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T06:40:34.643085Z",
     "start_time": "2020-12-16T06:40:34.629122Z"
    }
   },
   "outputs": [],
   "source": [
    "# a.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Textfile RDD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T03:08:54.676435Z",
     "start_time": "2020-12-21T03:08:54.520851Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BDSN software installation',\n",
       " '',\n",
       " 'https://drive.google.com/drive/folders/1e-x1mI6mfNnl1v3qZ_jgwfdauo39exxv',\n",
       " '',\n",
       " 'Gow-0.8.0.exe',\n",
       " 'jre-8u271-windows-x64.exe',\n",
       " 'spark-2.2.0-bin-hadoop2.7.zip',\n",
       " 'winutils.exe',\n",
       " '',\n",
       " 'setx SPARK_HOME C:\\\\sparkinstall\\\\spark-2.2.0-bin-hadoop2.7',\n",
       " 'setx HADOOP_HOME C:\\\\sparkinstall\\\\spark-2.2.0-bin-hadoop2.7',\n",
       " 'setx PYSPARK_DRIVER_PYTHON ipython',\n",
       " 'setx PYSPARK_DRIVER_PYTHON_OPTS notebook',\n",
       " '',\n",
       " 'spark-submit --version',\n",
       " '',\n",
       " 'C:\\\\Users\\\\Goutham-ROG\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39',\n",
       " '',\n",
       " 'C:\\\\sparkinstall\\\\spark-2.2.0-bin-hadoop2.7\\\\bin',\n",
       " '',\n",
       " 'pip install jupyter',\n",
       " '',\n",
       " '',\n",
       " 'goto ',\n",
       " 'C:\\\\sparkinstall\\\\spark-2.2.0-bin-hadoop2.7\\\\bin\\\\Spark Notebooks']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sc.textFile(\"software installation.txt\")\n",
    "data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T03:08:58.316892Z",
     "start_time": "2020-12-21T03:08:57.910977Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BDSN software installation',\n",
       " '',\n",
       " 'https://drive.google.com/drive/folders/1e-x1mI6mfNnl1v3qZ_jgwfdauo39exxv',\n",
       " '',\n",
       " 'Gow-0.8.0.exe',\n",
       " 'jre-8u271-windows-x64.exe']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(6) #take 6 lines from the dataRDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating key Value RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T03:54:57.793529Z",
     "start_time": "2020-12-21T03:54:57.670857Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0), (1, 1), (2, 4), (3, 9)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating a plain RDD\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "kv = sc.parallelize([(0,0),(1,1),(2,4),(3,9)])\n",
    "kv.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
